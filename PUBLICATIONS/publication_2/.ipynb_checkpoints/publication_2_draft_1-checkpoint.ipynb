{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nap9AVvupt6w"
   },
   "source": [
    "# Revisiting Food-Safety Inspections from the Chicago Dataset - A Tutorial (Part 1)\n",
    "## 1. Introduction\n",
    "Foodborne illnesses afflict 48 million Americans each year, resulting in 128,000 hospitalizations and 3,000 deaths [(CDC)](https://www.cdc.gov/foodborneburden/index.html). City governments curb the spread of illness by enforcing stringent health codes but often face daunting inspection schedules and severe budget constraints. To meet regulations both public officials and the businesses they inspect must make difficult decisions in allocating scarce resources.\n",
    "\n",
    "In this tutorial we present our efforts to support these decision-makers by predicting when and where health code violations will occur. As we draw from past research, clean and explore available datasets and ultimately build and test a predictive model of inspection outcomes we invite you to share any questions, comments or critiques that come to mind. This project would not be possible without generous community support and we welcome the opportunity to extend that community and expand our skills as developers and data scientists.\n",
    "\n",
    "### 1.1. Past Work\n",
    "In the city of Chicago health code violations are rated as minor, serious or critical. Critical violations denote a pressing hazard to public health and an automatic failed inspection, so a health inspector's job is essentially to minimize the duration for which critical violations go undiscovered.\n",
    "\n",
    "In 2014 the Chicago Department of Public Health (CDPH) began work on a [predictive model of inspection outcomes](https://chicago.github.io/food-inspections-evaluation/) to facilitate the discovery of critical violations. The team explored a number of datasets and features before eventually arriving at the following list of predictors:\n",
    "* inspector assigned\n",
    "* previous inspection outcomes\n",
    "* age at inspection\n",
    "* license for alcohol consumption on premises\n",
    "* license for over counter tobacco retail\n",
    "* local temperature at time of inspection\n",
    "* nearby burglaries\n",
    "* nearby sanitation complaints\n",
    "* nearby garbage cart requests.\n",
    "\n",
    "Using a glmnet model trained on these features the team was able to demonstrate that a data-driven inspections schedule could significantly reduce the time to find a critical violation. This improved inspections schedule has since been adopted by the CDPH and both the [model](https://github.com/Chicago/food-inspections-evaluation) and [data used](https://data.cityofchicago.org/) have been made publicly available.\n",
    "\n",
    "### 1.2. Premise\n",
    "In light of the CDPH's success we began this project as an exploration and extension of their work. Whereas the Chicago team set out to predict critical violations, we are working to develop a series of models predicting individual violations as well as to explore neural networks for the prediction model.\n",
    "\n",
    "By developing a higher resolution model we hope to provide actionable information not just to public officials but to restaurants themselves. In addition to a general interest in customers' health restaurants are fined \\$500 for each critical and $250 for each serious violation found, and so are strongly motivated to maintain health codes. A real-time violation risk analysis could thus provide actionable information to restaurants on an ongoing basis, improving inspection outcomes and promoting public health.\n",
    "\n",
    "### 1.3. Data\n",
    "Although we are developing a more specific model than the CDPH and using different tools to do our investigative question is similar. Thus we began by preparing those datasets and features that the Chicago team found to be significant. While we may draw from additional datasets as work progresses the current list is as follows:\n",
    "* [food inspections](https://data.cityofchicago.org/Health-Human-Services/Food-Inspections/4ijn-s7e5/data)\n",
    "* [business licenses](https://data.cityofchicago.org/Community-Economic-Development/Business-Licenses/r5kz-chrr/data)\n",
    "* [garbage cart requests](https://www.cityofchicago.org/city/en/dataset/garbage_cart_stolenormissing.html)\n",
    "* [sanitation code complaints](https://data.cityofchicago.org/Service-Requests/311-Service-Requests-Sanitation-Code-Complaints/me59-5fac/data)\n",
    "* [crimes](https://data.cityofchicago.org/Public-Safety/Crimes-2018/3i3m-jwuy/data)\n",
    "* [weather](https://darksky.net/dev)\n",
    "\n",
    "With the exception of weather data from darksky.net all data used is available through the [Chicago Data Portal](https://data.cityofchicago.org/). We will not be working with data on inspector assigned as this is not publicly available and so not useful in practice.\n",
    "\n",
    "## 2. Pre-processing and Data Structuring\n",
    "In order to derive insights from our data we first had to ensure that each dataset is consistent and ready for further processing (e.g. standardizing column names, removing duplicates, filtering unwanted data, converting types). After this preliminary cleaning we then had to translate certain observations into a form more convenient for analysis (e.g. splitting strings of violations into a series of binary violation columns). Finally we used this data to derive secondary features like the critical violation count or the time since last inspection. The results of these processes are [available on dropbox]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1B8UDMqF4PgH"
   },
   "source": [
    "### 2.1. Preparation of Food Inspections Data\n",
    "\n",
    "To download the inspections data we used Sodapy, a client for the Socrata Open Data API (the data platform used by the Chicago Data Portal):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install sodapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "n13gBW514xZm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Requests made without an app_token will be subject to strict throttling limits.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sodapy import Socrata\n",
    "\n",
    "# Unauthenticated client only works with public data sets. Note 'None'\n",
    "# in place of application token, and no username or password:\n",
    "client = Socrata(\"data.cityofchicago.org\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6-1emP0T97Dl"
   },
   "source": [
    "Conveniently,  Sodapy converts all columns to snake case, removes special characters and standardizes date format by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "PviTQIJJ99HA"
   },
   "outputs": [],
   "source": [
    "# First 50000 results, returned as JSON from API \n",
    "# Connverted to Python list of dictionaries by sodapy.\n",
    "# Column names converted to snake case, special chars removed\n",
    "# Dates and location formatted\n",
    "results = client.get(\"4ijn-s7e5\", limit=50000)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "inspections = pd.DataFrame.from_records(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v7UFnhQ37IsO"
   },
   "source": [
    "As Socrata restricts queries to 50000 entries we then paged through the remainder to access the full dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "25AARonl7DB_"
   },
   "outputs": [],
   "source": [
    "# Download remaining food inspections (limit 50000 / call)\n",
    "start = 50000\n",
    "while results:\n",
    "    results = client.get(\"4ijn-s7e5\", limit=50000, offset=start)\n",
    "    inspections = inspections.append(pd.DataFrame.from_records(results))\n",
    "    start += 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tBLirEmb-u2Z"
   },
   "source": [
    "After exploring the data and reading through the CDPH's work in R we then applied a series of filters to produce a consistent and usable dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "wsYlvAYH_Mqm"
   },
   "outputs": [],
   "source": [
    "# Remove trailing backslash (left over from sodapy conversion of \"License #\" column)\n",
    "inspections.rename(columns={\"license_\": \"license\"}, inplace=True)\n",
    "\n",
    "# Drop rows with missing data\n",
    "inspections.dropna(subset=[\"inspection_date\", \"license\", \"latitude\", \"longitude\"], inplace=True)\n",
    "\n",
    "# Convert latitude and longitude to float\n",
    "inspections.latitude = inspections.latitude.astype(float)\n",
    "inspections.longitude = inspections.longitude.astype(float)\n",
    "\n",
    "# Drop duplicates\n",
    "inspections.drop_duplicates(\"inspection_id\", inplace=True)\n",
    "\n",
    "# Drop \"0\" licenses\n",
    "inspections = inspections[inspections.license != \"0\"]\n",
    "\n",
    "# Only consider successful inspections\n",
    "inspections = inspections[~inspections.results.isin([\"Out of Business\", \"Business Not Located\", \"No Entry\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later calculations we converted location data to floats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert latitude & longitude to floats\n",
    "inspections.latitude = inspections.latitude.astype(float)\n",
    "inspections.longitude = inspections.longitude.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rW7v0L_Z_0Eh"
   },
   "source": [
    "In addition to semi-regular canvas inspections facilities may also be inspected due to complaints or a recent failed inspection. To ensure that our data is representative of restaurants operating as usual we filtered the dataset to consist only of canvas inspections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "S8-Co0wT_qf1"
   },
   "outputs": [],
   "source": [
    "# Only consider canvas inspections (not complaints or re-inspections)\n",
    "inspections = inspections[inspections.inspection_type == \"Canvass\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I1pCWb7dBDbd"
   },
   "source": [
    "We also filtered inspections by facility type, to eliminate the inconsistency of data from sources like hospitals or schools which follow different inspection schedules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "1Zojy2xVBEmd"
   },
   "outputs": [],
   "source": [
    "# Only consider restaurants and grocery stores (subject to change)\n",
    "inspections = inspections[inspections.facility_type.isin([\"Restaurant\", \"Grocery Store\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EMm8U_k_CURy"
   },
   "source": [
    "Finally we saved the resulting dataframe as a CSV file, ready for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "VnJQgnGSCUso"
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "root_path = os.path.dirname(os.getcwd())\n",
    "\n",
    "# Save result\n",
    "inspections.to_csv(os.path.join(root_path, \"DATA/food_inspections_demo.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E6e0XFCTGY-j"
   },
   "source": [
    "### 2.2. Preparation of Remaining Socrata Data\n",
    "The process of downloading and filtering business licenses, garbage cart requests, sanitation complaints and crimes was much the same as with food inspections - for specifics please see the [CODE folder of our project repository](https://github.com/Sustainabilist/ChicagoDataAnalysis/tree/master/CODE). \n",
    "\n",
    "In brief:\n",
    "* All datasets were formatted by sodapy and filtered to remove duplicates and missing data.\n",
    "* Crimes were filtered to include only burglaries since 2010 to reduce size and ensure consistency.\n",
    "* Garbage cart requests and sanitation complaints were filtered to include only completed or open requests.\n",
    "\n",
    "### 2.3. Preparation of Weather Data\n",
    "The process of pulling the weather data from the [Darksky API](https://darksky.net/dev) was straightforward and can be found in notebook 16 in the CODE folder of our project repository. To eliminate redundant API calls, the script first imports any existing weather data and checks this data first before each request.\n",
    "\n",
    "Each weather record consists of an inspection ID, a date, the pricipitation intensity, the maxiumum temperature, the windspeed and the humidity at that date. As the inspections region only measures some twenty miles across we used a single location in central Chicago for all weather records.\n",
    "\n",
    "### 2.4. Calculation of Violations Data\n",
    "Each entry in the violations column of the food inspections dataset is made up of a number of violation/comment pairs joined into a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'33. FOOD AND NON-FOOD CONTACT EQUIPMENT UTENSILS CLEAN, FREE OF ABRASIVE DETERGENTS - Comments: OBSERVED THE CUTTING BOARDS IN THE PREP AREA NOT CLEAN, INSTRUCTED TO CLEAN AND SANITIZE. ALSO THE LARGE BOX FREEZER NOT CLEAN, IN SIDE STORAGE, AND THE 2 DOOR FREEZER IN REAR PREP NOT CLEAN. INSTRUCTED TO CLEAN.  | 35. WALLS, CEILINGS, ATTACHED EQUIPMENT CONSTRUCTED PER CODE: GOOD REPAIR, SURFACES CLEAN AND DUST-LESS CLEANING METHODS - Comments: OBSERVED DUSTY CEILING VENT IN FRONT OF THE LADIES RESTROOM, INSTRUCTED TO CLEAN, ALSO CLEAN LIGHT SHIELD IN 1ST. LADIES RESTROOM, AND CLEAN THE WALL IN COOKING AREA. | 39. LINEN: CLEAN AND SOILED PROPERLY STORED - Comments: OBSERVED THE WIPING CLOTHES IMPROPERLY STORED  ON THE PREP TABLES IN THE REAR PREP AREA, INSTRUCTED TO STORE IN CLEAN CONTAINER OF SANITIZER.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspections.iloc[2].violations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this represents a rich source of data, this format makes it difficult to perform operations based on specific violations (e.g. charting the distribution of critical violations).\n",
    "\n",
    "To make violations data more accessible for analysis we split each entry into a series of columns representing the presence of each violation with binary flags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unicode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-92df27defd73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# 5 mins\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mvalues_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviolations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_violations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3190\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3191\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3192\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3194\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-92df27defd73>\u001b[0m in \u001b[0;36msplit_violations\u001b[0;34m(violations)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msplit_violations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mviolations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mvalues_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mviolations\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mviolations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mviolations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' | '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mviolation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mviolations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unicode' is not defined"
     ]
    }
   ],
   "source": [
    "# Split violations into binary values for each violation\n",
    "def split_violations(violations):\n",
    "    values_row = pd.Series([])\n",
    "    if type(violations) in [str, unicode]:\n",
    "        violations = violations.split(' | ')\n",
    "        for violation in violations:\n",
    "            index = \"v_\" + violation.split('.')[0]\n",
    "            values_row[index] = 1\n",
    "    return values_row\n",
    "\n",
    "# 5 mins\n",
    "values_data = inspections.violations.apply(split_violations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then generated a series of column titles of the form \"v_1\" for violations 1-14 (critical), 15-29 (serious) and 30-44 plus 70 (minor):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate column names\n",
    "critical_columns = [(\"v_\" + str(num)) for num in range(1, 15)]\n",
    "serious_columns = [(\"v_\" + str(num)) for num in range(15, 30)]\n",
    "minor_columns = [(\"v_\" + str(num)) for num in range(30, 45)]\n",
    "minor_columns.append(\"v_70\")\n",
    "\n",
    "columns = critical_columns + serious_columns + minor_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These column headings were then combined with the violation values and paired with the inspection id for each row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure no missing columns, fill NaN\n",
    "values = pd.DataFrame(values_data, columns=columns).fillna(0)\n",
    "\n",
    "values['inspection_id'] = inspections.inspection_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting datatable is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While not especially human-readable, this format allows us to easily access specific violation data, sum sets of violations and generate models that require numeric or binary features.\n",
    "\n",
    "Next we created a separate dataframe for critical, serious and minor violation counts by pairing each inspection id with the sum of the appropriate subset of violation values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count violations\n",
    "counts = pd.DataFrame({\n",
    "    \"critical_count\": values[critical_columns].sum(axis=1),\n",
    "    \"serious_count\": values[serious_columns].sum(axis=1),\n",
    "    \"minor_count\": values[minor_columns].sum(axis=1)\n",
    "})\n",
    "\n",
    "counts['inspection_id'] = inspections.inspection_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we saved both datasets for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save violation values and counts\n",
    "values.to_csv(os.path.join(root_path, \"DATA/violation_values_demo.csv\"), index=False)\n",
    "counts.to_csv(os.path.join(root_path, \"DATA/violation_counts_demo.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u1aMOOCrLLpC"
   },
   "source": [
    "## 3. Exploratory Analysis\n",
    "To familiarize ourselves with the data and to inform feature selection and model design we then took some time to explore and visualize. During this period we investigated the spatial distribution of inspections, the comments accompanying violations, the relative distribution of violations and the role of past inspection outcomes as predictors. We also looked at facility inspection statistics to assess the economic potential of violation forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Inspections Map\n",
    "To improve our understanading of the inspections dataset and its relation to other location-based data we used seaborn and folium to generate an inspections heat map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages \n",
    "import folium\n",
    "from folium import plugins\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate map\n",
    "m = folium.Map([41.8600, -87.6298], zoom_start=10)\n",
    "\n",
    "# Convert to (n, 2) nd-array format for heatmap\n",
    "inspections_arr = inspections.sample(20000)[[\"latitude\", \"longitude\"]].values\n",
    "\n",
    "# Plot heatmap\n",
    "m.add_child(plugins.HeatMap(inspections_arr.tolist(), radius=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Comments Wordcloud\n",
    "Wordclouds are an easy and surprisingly insightful means to explore text data, so to explore the feedback that facilities recieve alongside violations we concatenated all comments and used the resulting string to generate a wordcloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Extract comments from violations\n",
    "def get_comments(violations):\n",
    "    comments = \"\"\n",
    "    if type(violations) == str:\n",
    "        violations = violations.split(' | ')\n",
    "        for violation in violations:\n",
    "            violation = violation.split('Comments:')\n",
    "            if len(violation) == 2:\n",
    "                comments += violation[1]\n",
    "    return comments\n",
    "\n",
    "# Concatenate all comments\n",
    "comments = inspections.violations.apply(get_comments).str.cat(sep=\" \")\n",
    "\n",
    "# Generate wordcloud\n",
    "comments_wordcloud = WordCloud().generate(comments)\n",
    "\n",
    "# Plot wordcloud\n",
    "pylab.rcParams['figure.figsize'] = (10, 10)\n",
    "plt.imshow(comments_wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although qualitative the results point to maintenance, cleanliness, prep areas, sinks and coolers as common areas of concern.\n",
    "\n",
    "### 3.4. Violations Chart\n",
    "For a more quantitative perspective we used the violations data and [a chart of the Chicago health code](https://webapps1.cityofchicago.org/healthinspection/Code_Violations.jsp) to generate a simple bar graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List violation descriptions\n",
    "descriptions = {\"v_1\": \"Approved food sources (1)\", \"v_2\": \"Hot/cold storage facilities (2)\", \"v_3\": \"Hot/cold storage temp. (3)\", \"v_4\": \"Contaminant protection (4)\", \"v_5\": \"No sick handlers (5)\", \"v_6\": \"Proper hand washing (6)\", \"v_7\": \"Proper utensil washing (7)\", \"v_8\": \"Proper sanitizing solution (8)\", \"v_9\": \"Hot/cold water supply (9)\", \"v_10\": \"Waste water disposal (10)\", \"v_11\": \"Adequate toilet facilities (11)\", \"v_12\": \"Adequate hand washing facilities (12)\", \"v_13\": \"Control of rodents, other pests (13)\", \"v_14\": \"Correct serious violations (14)\", \"v_15\": \"No re-served food (15)\", \"v_16\": \"Protection from contamination (16)\", \"v_17\": \"Proper thawing (17)\", \"v_18\": \"Pest control, associated areas (18)\", \"v_19\": \"Proper garbage area (19)\", \"v_20\": \"Proper garbage storage (20)\", \"v_21\": \"Oversight of hazardous food (21)\", \"v_22\": \"Dishwasher maintenance (22)\", \"v_23\": \"Scrape before washing (23)\", \"v_24\": \"Proper dishwashers (24)\", \"v_25\": \"Minimize toxic materials (25)\", \"v_26\": \"Adequate customer toilets (26)\", \"v_27\": \"Supplied toilet facilities (27)\", \"v_28\": \"Visible inspection report (28)\", \"v_29\": \"Correct minor violations (29)\", \"v_30\": \"Labelled containers (30)\", \"v_31\": \"Sterile utensils (31)\", \"v_32\": \"Clean, maintain equipment (32)\", \"v_33\": \"Clean, sanitize utensils (33)\", \"v_34\": \"Clean, maintain floor (34)\", \"v_35\": \"Maintain walls & ceiling (35)\", \"v_36\": \"Proper lighting (36)\", \"v_37\": \"Toilet rooms vented (37)\", \"v_38\": \"Proper venting, plumbing (38)\", \"v_39\": \"Linen, clothing storage (39)\", \"v_40\": \"Proper thermometers (40)\", \"v_41\": \"Clean facilities, store supplies (41)\", \"v_42\": \"Ice handling, hairnets, clothes (42)\", \"v_43\": \"Ice equipment storage (43)\", \"v_44\": \"Restrict prep area traffic (44)\", \"v_70\": \"Restrict smoking (70)\"}\n",
    "\n",
    "# Rename with descriptions, sum binary values for each violation\n",
    "sums = values.drop(columns=[\"inspection_id\"]).rename(columns=descriptions).sum()\n",
    "\n",
    "# Generate color list\n",
    "colors = [\"red\"] * 15 + [\"orange\"] * 14 + [\"green\"] * 16\n",
    "\n",
    "# Sort sums and colors by sum value\n",
    "sum_data = pd.DataFrame({\"sums\": sums, \"colors\": colors}).sort_values(\"sums\")\n",
    "\n",
    "# Plot bar chart\n",
    "pylab.rcParams['figure.figsize'] = (10, 10)\n",
    "ax = sum_data.sums.plot(kind=\"barh\", color=sum_data.colors)\n",
    "ax.set_title(\"Health Code Violations\", fontsize=25)\n",
    "ax.set_xlabel(\"Violation Count\", fontsize=15)\n",
    "ax.invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this chart makes clear, the vast majority of violations are minor (v_30+), with only a scattering of serious (v_15-29) and critical (v_1-14) violations.\n",
    "\n",
    "The most common violations pertain to cleanliness and maintenance of floors, walls and ceilings; sanitation and maintenance of utensils and equipment; proper venting; and proper organization of food and cleaning supplies.\n",
    "\n",
    "The most common serious violation is improper control of vermin (v_18), and the most common critical violations relate to improper storage of hot and cold foods (v_2, v_3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g-WSIdpM4Nee"
   },
   "source": [
    "### 3.5. Inspection History\n",
    "The Chicago team found one of the greatest predictors of critical violations and failed inspections to be the establishment's recent inspection history. To investigate this we grouped inspections by license, shifted each group to find the previous inspection and set up a table to compare conditional likelihoods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort inspections by date\n",
    "inspections.sort_values(by=\"inspection_date\", inplace=True)\n",
    "\n",
    "# Group inspections by license and shift 1 to find previous results\n",
    "inspections[\"previous_results\"] = inspections.groupby(by=\"license\").shift().results\n",
    "\n",
    "# Calculate cross tabulation of results and previous results\n",
    "pd.crosstab(inspections.previous_results, inspections.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P(fail | fail)\n",
    "2416 / len(inspections[inspections[\"previous_results\"] == \"Fail\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P(fail | pass)\n",
    "4800 / len(inspections[inspections[\"previous_results\"] == \"Pass\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that facilities with a previous failure were almost twice as likely to fail as those with previous passing inspections, supporting the findings of the Chicago team."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Economic Impact\n",
    "Before investing time in a violations forecasting tool for businesses we first set out to assess whether such information would actually be valuable. If businesses rarely fail inspections and generally do so in summer, for example, even perfect information would not provide much additional value. If on the other hand failed inspections are frequent and difficult to predict, such a forecast would be valuable and actionable.\n",
    "\n",
    "To explore this we first grouped inspections by license (as all inspections for a facility share a common license) and then for each license group determined the age, yearly inspections, failure rate and several other statistics:\n",
    "\n",
    "* discuss fines\n",
    "* don't promise anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert inspection_date to datetime format\n",
    "inspections[\"datetime\"] = pd.to_datetime(inspections[\"inspection_date\"])\n",
    "\n",
    "# Sort by date\n",
    "inspections.sort_values(\"datetime\", inplace=True)\n",
    "\n",
    "# Calculate statistics for license groups\n",
    "def get_stats(group):\n",
    "    inspections = len(group)\n",
    "    fails = len(group[group.results == 'Fail'])\n",
    "    most_recent = group.iloc[-1].datetime\n",
    "    days = (most_recent - group.iloc[0].datetime).days + 1\n",
    "    years = days / 365.25\n",
    "    return pd.Series({\n",
    "        'inspections': inspections,\n",
    "        'fails': fails,\n",
    "        'most_recent': most_recent,\n",
    "        'days': days,\n",
    "        'years': years\n",
    "    })\n",
    "\n",
    "# Group by license and apply get_stats\n",
    "licenses = inspections.groupby('license').apply(get_stats).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin we plotted the number licenses by age bracket, and found that after a huge number of licenses with only one inspection (years_old = 0) the die off is gradual:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore whether businesses fail enough inspections for risk analysis to be worthwhile we plotted the licenses in each yearly fails bracket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count restaurants in each rounded age bracket and plot\n",
    "age_counts = licenses.years.round().value_counts().sort_index()\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = (10, 7)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "index = np.arange(len(age_counts))\n",
    "ax1.bar(index, age_counts)\n",
    "ax1.plot(age_counts.cumsum())\n",
    "ax1.set_xticks(index)\n",
    "ax1.set_title(\"Inspection History\", fontsize=20)\n",
    "ax1.set_xlabel(\"Years Inspected\", fontsize=15)\n",
    "ax1.set_ylabel(\"Number of Businesses\", fontsize=15)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_yticks([1, 2, 3, 4, 5, 5.25])\n",
    "ax2.set_yticklabels([\"%d%%\"%(x*20) for x in range(1,6)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate fails per rounded year\n",
    "yearly_fails = licenses.fails / np.ceil(licenses.years)\n",
    "\n",
    "# Count restaurants in each rounded yearly fails bracket\n",
    "yearly_fail_counts = yearly_fails.round(1).value_counts().sort_index()\n",
    "\n",
    "index = np.arange(len(yearly_fail_counts))\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.bar(index, yearly_fail_counts)\n",
    "ax1.plot(index, yearly_fail_counts.cumsum())\n",
    "ax1.set_xticks(index)\n",
    "ax1.set_xticklabels(yearly_fail_counts.index)\n",
    "ax1.set_title(\"Yearly Inspection Fails\", fontsize=20)\n",
    "ax1.set_xlabel(\"Yearly Fails\", fontsize=15)\n",
    "ax1.set_ylabel(\"Number of Businesses\", fontsize=15)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_yticks([1, 2, 3, 4, 5, 5.25])\n",
    "ax2.set_yticklabels([\"%d%%\"%(x*20) for x in range(1,6)])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that after a huge number of facilities with a 100% pass rate there is a gradual die-off in yearly fails, with some 2,000 out of 15,000 businesses failing one or more yearly inspections. \n",
    "\n",
    "Finally we computed a cross-tabulation of failure rate and business age to explore performance and survival over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart failure rate vs. age\n",
    "pd.crosstab(licenses.years_old.round(), licenses.failure_rate.round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select yearly fails series\n",
    "yearly_fails = licenses.yearly_fails\n",
    "\n",
    "# Count licenses and licenses failing 1 or more inspections yearly\n",
    "print(len(licenses), len(yearly_fails[yearly_fails >= 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results suggest a high failure rate and die-off rate for businesses early on, transitioning to a lower failure rate and die-off rate for more established businesses. Early inspection failures could by a cause of closure, a symptom of adverse conditions or both.\n",
    "\n",
    "In summary we do believe that there is sufficient opportunity for a risk analysis tool to justify development. A failed inspection costs a business at minimum $500 in fines and represents a serious risk to public health. For new and potentially struggling businesses especially an advanced assessment of health and inspection risks could prove highly valuable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ZdHzXcM5ith"
   },
   "source": [
    "## 5. Next Steps\n",
    "[UNDECIDED]\n",
    "* Kernel density estimation\n",
    "* Selection of features\n",
    "* Calculation of model data\n",
    "* Process engineering? Control charts?\n",
    "* neural net for critical violations\n",
    "* compare results to CDPH's model\n",
    "* neural net for individual violations\n",
    "* look at which things can actually be predicted and would provide actionable info\n",
    "* develop forecast tool\n",
    "* disseminate forecast tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hlarm5KB6I_t"
   },
   "source": [
    "## About the Authors\n",
    "[NEEDS SOMETHING ABOUT JASON/SUSTAINABILIST?]\n",
    "* David Lewis is a seasoned corporate responsibility professional working to utilize technology to help improve the health and well being of human populations through environmental stewardship. \n",
    "* Russell Hofvendahl is a web application developer with a deep-seated love of data driven decision making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revisiting Food-Safety Inspections from the Chicago Dataset - A Tutorial (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. In Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Kernel Density Estimation\n",
    "[HAVEN'T WRITTEN THE CODE FOR THIS]\n",
    "* gaussian kde\n",
    "* 90 day window\n",
    "* kde for garbage, crime, sanitation\n",
    "* call something other than heat map to avoid confuion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VRZ0Gd9aQwBv"
   },
   "source": [
    "## 3. Calculation of Model Data\n",
    "We then set out derive all features originally considered by the Chicago team plus several novel features, translating from R to Python in the process. While relative weights in forecasting individual violations rather than number of critical violations may be different, we expect the significant databases and [NEEDS REPHRASING & STUFF]\n",
    "\n",
    "[MAYBE HAVE BULLET POINTS FOR DATABASES, FEATURES]\n",
    "\n",
    "Each row of the final training dataset represents an inspection, its outcomes, and a number of other features. [SOMETHING ABOUT TRAINING METHODOLOGY, MAYBE ABOUT FIGURING OUT WHICH FEATURES MATTER FOR WHICH]\n",
    "\n",
    "We used the inspections database as the basis for our training data.\n",
    "\n",
    "List of features is as follows:\n",
    "\n",
    "### 3.1. features from food inspection data\n",
    "[NEED TO WRITE THIS UP]\n",
    "* used subset of inspections as basis\n",
    "* merged with violation values & counts on inspection id\n",
    "* created pass / fail flags\n",
    "* sorted by date, shifted to get previous info\n",
    "  * past fail\n",
    "  * past critical, serious, minor\n",
    "  * inspections with no past default to 0\n",
    "* calculate time since last\n",
    "  * binary first_record\n",
    "  * time since last defaults to 2\n",
    "  \n",
    "### 3.2. features from business license data\n",
    "[YET TO BE CALCULATED]\n",
    "  \n",
    "### 3.3. from heat maps\n",
    "[YET TO BE CALCULATED]\n",
    "  \n",
    "### 3.4. from weather\n",
    "[YET TO BE CALCULATED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generating Model"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "publication_1",
   "provenance": [
    {
     "file_id": "1cr478KRLrD-8amy4zC7yPwnQfaPFcpts",
     "timestamp": 1529284676149
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
